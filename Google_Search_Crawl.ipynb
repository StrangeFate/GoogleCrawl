{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc977704-188f-48be-82b2-b305fc4d31fe",
   "metadata": {
    "id": "overall-terrorism",
    "tags": []
   },
   "source": [
    "# 사용설명서 #\n",
    "\n",
    "작성자: 김기연 \n",
    "\n",
    "\n",
    "업데이트: 2021-08-15(16시)\n",
    "\n",
    "\n",
    "연락처: h20161202@g-mail.hallym.ac.kr OR Strangefate.k@gmail.com\n",
    "\n",
    " * 사용 전 필수 정독 사항\n",
    "   - 본 파일은 윈도우 운영체제 기반으로 작성되었습니다.\n",
    "   - 본 파일은 콜랩 환경을 지원하지 않습니다. 대신, 본 파일이 속한 폴더 전체를 다운로드 받아 실행시켜야 합니다.\n",
    "   - 본 파일에는 다양한 라이브러리가 사용되었습니다. 사용된 라이브러리는 다음과 같습니다.\n",
    "     - 필수 목록: selenium, pandas, openpyxl\n",
    "   - 본 파일 사용을 위해서는 필수 목록의 라이브러리 설치가 필요합니다. 라이브러리 설치법은 다음과 같습니다.\n",
    "     - 명령 프롬프트 이용\n",
    "     1. cmd창을 연다(윈도우키 + R 동시에 눌러 나오는 창에 cmd를 입력)\n",
    "     2. pip install 라이브러리이름 을 입력한다.\n",
    "     3. 설치를 기다린다.\n",
    "     - 쥬피터 노트북 이용\n",
    "     1. 빈 코드셀을 만든다.\n",
    "     2. pip install 라이브러리이름 을 입력한다.\n",
    "   - 다만, 개발환경에 따라 위 두 방법이 작동하지 않을 수도 있습니다. 이 경우, 개발환경에 맞는 방식의 라이브러리 설치방식을 사용하셔야 합니다.\n",
    "   - 이후 하단에 위치한 라이브러리 임포트 과정에서 문제가 발생이 없음을 확인 후 하단의 코드를 사용하시기 바랍니다.\n",
    "   - 기능을 이용하기 위해서는 모든 함수 선언 셀(def 로 시작하는)을 실행시켜 주셔야 합니다. 이후 해당 파트의 마지막에 표기된 실행 코드를 이용하시면 됩니다.\n",
    "   - 크롤링 데이터는 엑셀 내 시트 추가로 이루어집니다. 만약 동일한 키워드를 사용한 크롤링 데이터가 존재한다면 엑셀 파일 내 \"키워드1\" 와 같은 방식으로 새로운 시트를 생성합니다.\n",
    "   \n",
    "  * 에러 발생시 조치 사항\n",
    "    1. 하단의 라이브러리가 제대로 임포트 되었는지 확인하십시오.\n",
    "    2. 현재 파일이 존재하는 폴더 내에 chromedriver.exe가 존재하는지 확인하십시오.\n",
    "    3. chromedriver.exe와 사용자 컴퓨터 환경 내의 크롬 브라우저 버전이 동일한지 확인하십시오. 크롬 브라우저의 업데이트로 인해 기존 chromedriver.exe와 버전 충돌이 일어날 수 있습니다. chromedriver.exe를 업데이트 하십시오.\n",
    "    4. 상단부터 하단으로 코드를 실행시켜보십시오.\n",
    "    5. 이상의 조치에도 문제가 발생하면 상단의 연락처로 연락주시기 바랍니다.\n",
    "  \n",
    "   ----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "willing-conservation",
   "metadata": {
    "id": "demanding-treasure"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d6bf4c-74ee-4c59-82db-72bd7c95f0ad",
   "metadata": {
    "id": "hungarian-tract"
   },
   "source": [
    "## 구글 검색결과 크롤링 ##\n",
    "\n",
    "* 사용 방법\n",
    "  0. 폴더내에 키워드를 포함한 엑셀 파일이 있는지 확인해주세요.\n",
    "      - 엑셀 파일 이름은 \"키워드.xlsx\"로 끝나야 합니다(eg. 10월크롤링요구키워드.xlsx)\n",
    "      - 엑셀 파일 내의 첫번째 열 첫번째 행에는 Keyword 가 입력되어 있어야 합니다.\n",
    "      - 첫번째 열 두번째 행 부터는 검색결과 크롤링이 필요한 키워드를 입력하시면 됩니다.\n",
    "      - example키워드.xlsx를 참고하시기바랍니다.\n",
    "      \n",
    "      \n",
    "  1. 하단에 셀이 위치한 순서대로 실행시켜주세요.\n",
    "  2. 마지막 셀에서 web_crawler() 함수를 실행시키면 작동을 시작합니다.\n",
    "  3. 각 단계별로 진행상황이 시간과 함께 나타납니다. 오류 발생시 이를 확인해주세요.\n",
    "  4. DONE 메세지가 출력되면 폴더 내에 \"크롤링데이터.xlsx\"로 끝나는 이름의 파일이 생성됩니다.\n",
    "  5. 엑셀 파일을 확인해주세요.\n",
    "  \n",
    "* 중요 참고사항\n",
    "  - 키워드를 포함한 엑셀 파일이 없다면 아래 기능은 작동하지 않습니다.\n",
    "  - 크롤링을 위해 열리는 크롬 브라우저는 화면에 보이지 않는 옵션이 적용되어있습니다. 이를 해제하기 위해서는 아래 코드의 주석을 참고하세요.\n",
    "  - 크롤링은 검색결과 10페이지까지 진행됩니다.\n",
    "\n",
    "  ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adapted-hungary",
   "metadata": {
    "id": "alone-scientist"
   },
   "outputs": [],
   "source": [
    "#폴더 내 어플 링크 txt파일 읽어오는 함수\n",
    "def alexcelread() :\n",
    "    path = \"./\" #현재 디렉토리\n",
    "    file_list = os.listdir(path)\n",
    "    keyword_list_excel = [file for file in file_list if file.endswith(\"키워드.xlsx\")] #키워드.xlsx로 끝나는 파일 찾기\n",
    "    return keyword_list_excel #파일 리스트 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cdf6c2d-9b07-4b0d-bb2b-01a963158348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Pages(x) :\n",
    "    webdriver_options = webdriver.ChromeOptions()\n",
    "    webdriver_options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\") #크롤링 감지 방지를 위한 user-agent 수정\n",
    "    webdriver_options.add_argument('headless') #크롬 브라우저 띄우지 않는 옵션. 크롤링 화면을 보고 싶다면 #을 이용해 주석처리 할 것\n",
    "    driver = webdriver.Chrome('./chromedriver.exe', options=webdriver_options)\n",
    "    \n",
    "    url = \"https://www.google.com/search?q=\" + x #키워드인 x를 받아 해당 키워드에 대한 검색결과 URL을 생성함\n",
    "    \n",
    "    driver.get(url) #화면 띄우기\n",
    "    \n",
    "    page_to_return = [] #검색결과 페이지 링크를 저장할 리스트\n",
    "    \n",
    "    pages = driver.find_elements_by_xpath(\"//tr[@jsname='TeSSVd']/td/a\") #검색결과 페이지 링크\n",
    "    \n",
    "    page_to_return.append(url) #pages를 통해 불러온 링크는 1페이지 링크를 포함하지 않음. 1페이지 링크 추가\n",
    "    \n",
    "    for i in range(len(pages)) :\n",
    "        page_to_return.append(pages[i].get_attribute('href')) #나머지 검색페이지 링크 추가\n",
    "    \n",
    "    return page_to_return #검색페이지 10개 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2af44375-929e-46f8-a9e9-d4dc977d1fb6",
   "metadata": {
    "id": "permanent-plasma"
   },
   "outputs": [],
   "source": [
    "#크롤링을 진행하는 함수\n",
    "\n",
    "def crawlerX(x) : \n",
    "    webdriver_options = webdriver.ChromeOptions()\n",
    "    webdriver_options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\") #크롤링 감지 방지를 위한 user-agent 수정\n",
    "    webdriver_options.add_argument('headless') #크롬 브라우저 띄우지 않는 옵션. 크롤링 화면을 보고 싶다면 #을 이용해 주석처리 할 것\n",
    "    driver = webdriver.Chrome('./chromedriver.exe', options=webdriver_options)\n",
    "    \n",
    "    driver.get(x) #화면 띄우기\n",
    "    \n",
    "    total = [] #Name과 URL을 합칠 변수 선언\n",
    "    \n",
    "    URL = driver.find_elements_by_xpath(\"//div[@class='yuRUbf']/a\") #링크 불러오기\n",
    "    Name = driver.find_elements_by_xpath(\"//div[@class='yuRUbf']/a/h3\") #링크 별 제목 불러오기\n",
    "    \n",
    "    for i in range(len(Name)) :\n",
    "        total.append({ 'Name' : str(Name[i].text),\n",
    "                 'URL' : str(URL[i].get_attribute('href')) })\n",
    "\n",
    "    driver.quit() #웹드라이버 종료 및 크롬 창 종료. 메모리 관리 목적.\n",
    "    \n",
    "    return pd.DataFrame(total) #크롤링된 데이터 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "developing-lighter",
   "metadata": {
    "id": "retained-evolution"
   },
   "outputs": [],
   "source": [
    "#전체 함수를 조합한 함수(실행해야 하는 함수)\n",
    "def web_crawler() :\n",
    "    \n",
    "    print(\"Start\", datetime.today().strftime(\"%Y/%m/%d %H:%M:%S\"))\n",
    "    \n",
    "    total_df = pd.DataFrame([]) #저장을 위한 데이터 프레임 우선 선언\n",
    "    excel_list = alexcelread() #엑셀 파일 리스트 불러오기\n",
    "    \n",
    "    for a in excel_list : #엑셀 파일 리스트에서 하나씩 불러온다 = a\n",
    "        \n",
    "        print(a + \" is opened\", datetime.today().strftime(\"%Y/%m/%d %H:%M:%S\"))\n",
    "        \n",
    "        excel = pd.read_excel(a) #a라는 엑셀 파일을 열어 excel로 선언한다\n",
    "        keyword_list = excel[\"Keyword\"] #excel에서 키워드를 불러와 keyword_list에 저장한다\n",
    "        \n",
    "        for i in keyword_list : #keyword_list에서 키워드를 하나씩 불러온다 = i\n",
    "            \n",
    "            print(str(i) + \" keyword crawling is started\", datetime.today().strftime(\"%Y/%m/%d %H:%M:%S\"))\n",
    "            \n",
    "            for x in Get_Pages(i) : #Get_Pages에 키워드(i)를 넣는다. Get_Pages에서는 키워드에 대한 검색결과 링크를 리스트로 반환한다. 검색결과 링크 리스트를 하나씩 불러온다 = x\n",
    "                total_df = pd.concat([total_df, crawlerX(x)]) #crawlerX 함수에 검색결과 링크(x)를 입력, 크롤링 데이터가 포함된 딕셔너리를 받아온다. 이후 total_df와 합친다(10개 검색 페이지 결과를 합치기 위함)\n",
    "                time.sleep(5) #다음 검색결과 페이지로 진행하기 전 5초를 기다린다(리소스 관리목적, 사용 환경에 따라 조절 가능)\n",
    "            total_df.reset_index(drop=True, inplace = True) #인덱스를 초기화 한다\n",
    "            \n",
    "            if not os.path.exists(a[:-5] + \"크롤링데이터.xlsx\") : #엑셀 파일 이름은 엑셀 파일 이름에 크롤링데이터를 붙인 방식\n",
    "                with pd.ExcelWriter(a[:-5] + \"크롤링데이터.xlsx\", mode='w', engine='openpyxl') as writer : #엑셀 파일 없으면 생성\n",
    "                    total_df.to_excel(writer, sheet_name = str(i)) #시트 이름은 각 키워드 이름\n",
    "            else: \n",
    "                with pd.ExcelWriter(a[:-5] + \"크롤링데이터.xlsx\", mode='a', engine='openpyxl') as writer : #엑셀 파일 있으면 해당 파일 내 시트 추가\n",
    "                    total_df.to_excel(writer, sheet_name = str(i)) #시트 이름은 각 키워드 이름 이름\n",
    "            \n",
    "            total_df = pd.DataFrame([]) #딕셔너리를 초기화해 다른 키워드의 크롤링을 준비한다.\n",
    "                \n",
    "    print(\"DONE\", datetime.today().strftime(\"%Y/%m/%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51725f61-99e0-43bd-bf98-1732fcfaaeaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start 2021/08/15 16:07:42\n",
      "example키워드.xlsx is opened 2021/08/15 16:07:42\n",
      "파이썬 keyword crawling is started 2021/08/15 16:07:42\n",
      "깃허브 keyword crawling is started 2021/08/15 16:09:56\n",
      "한림대학교 keyword crawling is started 2021/08/15 16:12:07\n",
      "DONE 2021/08/15 16:14:21\n"
     ]
    }
   ],
   "source": [
    "web_crawler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-peeing",
   "metadata": {
    "id": "sustained-connection"
   },
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "프로젝트_함수모음.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
